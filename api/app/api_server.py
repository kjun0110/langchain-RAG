"""
FastAPI ë°±ì—”ë“œ ì„œë²„ - FastAPIì™€ LangChain ì—°ë™.

ì´ ì„œë²„ëŠ” FastAPIë¥¼ í†µí•´ LangChain RAG ì²´ì¸ì„ ì œê³µí•˜ëŠ” API ì„œë²„ì…ë‹ˆë‹¤.
worker ì„œë¹„ìŠ¤(app.py)ê°€ ì´ˆê¸°í™”í•œ pgvector ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬
ì±—ë´‡ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì—­í• :
- FastAPI ì„œë²„ ì œê³µ (REST API)
- LangChain RAG ì²´ì¸ ì‹¤í–‰
- workerê°€ ì´ˆê¸°í™”í•œ pgvector ë²¡í„° ìŠ¤í† ì–´ í™œìš©
"""

import os
import time
import warnings
from pathlib import Path
from typing import List, Optional

# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì°¾ê¸°)
try:
    from dotenv import load_dotenv

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (api/app/ -> api/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸)
    current_file = Path(__file__)
    project_root = current_file.parent.parent.parent
    env_file = project_root / ".env"

    if env_file.exists():
        load_dotenv(env_file)
    else:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ ì‹œë„
        load_dotenv()
except ImportError:
    pass  # python-dotenvê°€ ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ë§Œ ì‚¬ìš©

# PGVectorì˜ JSONB deprecation ê²½ê³  ë¬´ì‹œ
try:
    from langchain_core._api.deprecation import LangChainPendingDeprecationWarning

    warnings.filterwarnings(
        "ignore",
        category=LangChainPendingDeprecationWarning,
        module="langchain_community.vectorstores.pgvector",
    )
except ImportError:
    warnings.filterwarnings(
        "ignore",
        category=DeprecationWarning,
        module="langchain_community.vectorstores.pgvector",
    )

warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
    module="langchain_community.vectorstores.pgvector",
)

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from langchain_classic.chains import (
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from pydantic import BaseModel

# Neon PostgreSQL ì—°ê²° ë¬¸ìì—´ (.env íŒŒì¼ì˜ DATABASE_URL ì‚¬ìš©)
DATABASE_URL = os.getenv("DATABASE_URL")
SSLMODE = os.getenv("sslmode", "require")

if DATABASE_URL:
    # DATABASE_URLì— sslmodeê°€ ì—†ìœ¼ë©´ ì¶”ê°€
    if "sslmode=" not in DATABASE_URL:
        separator = "&" if "?" in DATABASE_URL else "?"
        CONNECTION_STRING = f"{DATABASE_URL}{separator}sslmode={SSLMODE}"
    else:
        CONNECTION_STRING = DATABASE_URL
else:
    # ê¸°ë³¸ê°’ (fallback)
    CONNECTION_STRING = os.getenv(
        "POSTGRES_CONNECTION_STRING",
        "postgresql://neondb_owner:npg_bNXv7Ll1mrBJ@ep-empty-tree-a15rzl4v-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require",
    )

COLLECTION_NAME = "langchain_collection"

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LangChain Chatbot API",
    description="PGVectorì™€ ì—°ë™ëœ LangChain ì±—ë´‡ API",
    version="1.0.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
vector_store: Optional[PGVector] = None
openai_embeddings = None
local_embeddings = None
openai_llm = None
local_llm = None
openai_rag_chain: Optional[Runnable] = None
local_rag_chain: Optional[Runnable] = None
# í• ë‹¹ëŸ‰ ì´ˆê³¼ ì¶”ì 
openai_quota_exceeded = False


def wait_for_postgres(max_retries: int = 30, delay: int = 2) -> None:
    """Neon PostgreSQLì´ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°."""
    import psycopg2

    print(
        f"[INFO] Neon PostgreSQL ì—°ê²° ì‹œë„ ì¤‘... (ì—°ê²° ë¬¸ìì—´: {CONNECTION_STRING[:50]}...)"
    )

    for i in range(max_retries):
        try:
            conn = psycopg2.connect(CONNECTION_STRING)

            # PGVector í™•ì¥ í™•ì¸
            cur = conn.cursor()
            cur.execute(
                "SELECT extname, extversion FROM pg_extension WHERE extname = 'vector'"
            )
            vector_ext = cur.fetchone()

            if vector_ext:
                print("[OK] Neon PostgreSQL ì—°ê²° ì„±ê³µ!")
                print(f"[INFO] PGVector í™•ì¥ ì„¤ì¹˜ë¨ (ë²„ì „: {vector_ext[1]})")
            else:
                print("[OK] Neon PostgreSQL ì—°ê²° ì„±ê³µ!")
                print("[WARNING] PGVector í™•ì¥ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

            conn.close()
            return
        except Exception as e:
            if i < max_retries - 1:
                print(
                    f"[INFO] Neon PostgreSQL ëŒ€ê¸° ì¤‘... ({i + 1}/{max_retries}) - {str(e)[:100]}"
                )
                time.sleep(delay)
            else:
                raise ConnectionError(f"Neon PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")


def initialize_embeddings():
    """Embedding ëª¨ë¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”."""
    global openai_embeddings, local_embeddings, openai_quota_exceeded
    openai_api_key = os.getenv("OPENAI_API_KEY")

    # OpenAI Embedding ì´ˆê¸°í™”
    if openai_api_key and openai_api_key != "your-api-key-here":
        try:
            openai_embeddings = OpenAIEmbeddings()
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            openai_embeddings.embed_query("test")
            print("[OK] OpenAI Embedding ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            error_msg = str(e)
            if (
                "quota" in error_msg.lower()
                or "429" in error_msg
                or "insufficient_quota" in error_msg
            ):
                openai_quota_exceeded = True
                print(f"[WARNING] OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼: {error_msg[:100]}...")
                print("   OpenAI Embeddingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                openai_embeddings = None
            else:
                print(f"[WARNING] OpenAI Embedding ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:100]}...")
                openai_embeddings = None
    else:
        print("[WARNING] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        openai_embeddings = None

    # ë¡œì»¬ Embedding ì´ˆê¸°í™”
    try:
        # langchain-huggingface ì‚¬ìš© (deprecation ê²½ê³  í•´ê²°)
        try:
            from langchain_huggingface import HuggingFaceEmbeddings
        except ImportError:
            # fallback to langchain_community
            from langchain_community.embeddings import HuggingFaceEmbeddings

        local_embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        local_embeddings.embed_query("test")
        print("[OK] ë¡œì»¬ Embedding ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (sentence-transformers)")
    except Exception as local_error:
        print(f"[WARNING] ë¡œì»¬ Embedding ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(local_error)[:100]}...")
        local_embeddings = None

    if not openai_embeddings and not local_embeddings:
        raise RuntimeError(
            "OpenAIì™€ ë¡œì»¬ Embedding ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
            "OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ sentence-transformersë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
        )


def initialize_llm():
    """LLM ëª¨ë¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”."""
    global openai_llm, local_llm, openai_quota_exceeded
    openai_api_key = os.getenv("OPENAI_API_KEY")

    # OpenAI LLM ì´ˆê¸°í™”
    if openai_api_key and openai_api_key != "your-api-key-here":
        try:
            openai_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
            # ì‹¤ì œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (í• ë‹¹ëŸ‰ í™•ì¸)
            openai_llm.invoke("test")
            print("[OK] OpenAI Chat ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            error_msg = str(e)
            if (
                "quota" in error_msg.lower()
                or "429" in error_msg
                or "insufficient_quota" in error_msg
            ):
                openai_quota_exceeded = True
                print(f"[WARNING] OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼: {error_msg[:100]}...")
                print("   OpenAI LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                openai_llm = None
            else:
                print(f"[WARNING] OpenAI Chat ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:100]}...")
                openai_llm = None
    else:
        print("[WARNING] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        openai_llm = None

    # ë¡œì»¬ Midm LLM ì´ˆê¸°í™”
    try:
        from app.model.midm import load_midm_model

        # .env íŒŒì¼ì—ì„œ LOCAL_MODEL_DIR ì½ê¸°
        local_model_dir = os.getenv("LOCAL_MODEL_DIR")
        if local_model_dir:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            from pathlib import Path

            if not Path(local_model_dir).is_absolute():
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                project_root = Path(__file__).parent.parent.parent
                local_model_dir = str(project_root / local_model_dir)
            print(f"[INFO] ë¡œì»¬ ëª¨ë¸ ë””ë ‰í† ë¦¬: {local_model_dir}")
            midm_model = load_midm_model(
                model_path=local_model_dir, register=False, is_default=False
            )
        else:
            midm_model = load_midm_model(register=False, is_default=False)

        local_llm = midm_model.get_langchain_model()
        print("[OK] ë¡œì»¬ Midm LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as local_error:
        error_msg = str(local_error)
        print(f"[WARNING] ë¡œì»¬ Midm ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:200]}...")
        import traceback

        print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()[:500]}")
        local_llm = None

    if not openai_llm and not local_llm:
        raise RuntimeError(
            "OpenAIì™€ ë¡œì»¬ LLM ëª¨ë¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
            "OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ Midm ëª¨ë¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        )


def initialize_vector_store():
    """PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™”."""
    global vector_store, openai_embeddings, local_embeddings

    # LLM_PROVIDERì— ë”°ë¼ ì ì ˆí•œ embedding ì„ íƒ
    llm_provider = os.getenv("LLM_PROVIDER", "openai").lower()

    # ì‚¬ìš©í•  embedding ëª¨ë¸ ì„ íƒ (ìš°ì„ ìˆœìœ„: LLM_PROVIDERì— ë§ëŠ” ëª¨ë¸ > OpenAI > ë¡œì»¬)
    if llm_provider == "midm" and local_embeddings:
        current_embeddings = local_embeddings
        print("[INFO] ë¡œì»¬ Embedding ëª¨ë¸ ì‚¬ìš© (LLM_PROVIDER=midm)")
    elif openai_embeddings:
        current_embeddings = openai_embeddings
        print("[INFO] OpenAI Embedding ëª¨ë¸ ì‚¬ìš©")
    elif local_embeddings:
        current_embeddings = local_embeddings
        print("[INFO] ë¡œì»¬ Embedding ëª¨ë¸ ì‚¬ìš© (fallback)")
    else:
        raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ Embedding ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    try:
        print("[INFO] ===== PGVector ì—°ê²° í™•ì¸ ì‹œì‘ =====")
        print(f"[INFO] ì»¬ë ‰ì…˜ ì´ë¦„: {COLLECTION_NAME}")
        print(f"[INFO] ì—°ê²° ë¬¸ìì—´: {CONNECTION_STRING[:60]}...")

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆê³  ë²¡í„° ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        try:
            print("[INFO] PGVector ê°ì²´ ìƒì„± ì¤‘...")
            vector_store = PGVector(
                embedding_function=current_embeddings,
                collection_name=COLLECTION_NAME,
                connection_string=CONNECTION_STRING,
            )
            print("[OK] PGVector ê°ì²´ ìƒì„± ì™„ë£Œ")

            # ë²¡í„° ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            import psycopg2

            print("[INFO] ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë²¡í„° ë°ì´í„° í™•ì¸ ì¤‘...")
            conn = psycopg2.connect(CONNECTION_STRING)
            cur = conn.cursor()

            # ì»¬ë ‰ì…˜ UUID í™•ì¸
            cur.execute(
                f"""
                SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
            """
            )
            collection_result = cur.fetchone()

            if collection_result:
                collection_uuid = collection_result[0]
                print(f"[INFO] ì»¬ë ‰ì…˜ UUID: {collection_uuid}")

                # ë²¡í„° ê°œìˆ˜ í™•ì¸
                cur.execute(
                    f"""
                    SELECT COUNT(*)
                    FROM langchain_pg_embedding
                    WHERE collection_id = '{collection_uuid}'
                """
                )
                vector_count = cur.fetchone()[0]

                # ë²¡í„° ì°¨ì› í™•ì¸
                cur.execute(
                    f"""
                    SELECT array_length(embedding::vector, 1) as vector_dim
                    FROM langchain_pg_embedding
                    WHERE collection_id = '{collection_uuid}'
                    LIMIT 1
                """
                )
                dim_result = cur.fetchone()
                vector_dim = dim_result[0] if dim_result and dim_result[0] else None

                conn.close()

                if vector_count > 0:
                    print("[OK] ê¸°ì¡´ PGVector ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
                    print(f"[INFO] ë²¡í„° ë°ì´í„° ê°œìˆ˜: {vector_count}ê°œ")
                    if vector_dim:
                        print(f"[INFO] ë²¡í„° ì°¨ì›: {vector_dim}ì°¨ì›")
                    print("[OK] ===== PGVector ì—°ê²° í™•ì¸ ì™„ë£Œ =====")
                else:
                    # ì»¬ë ‰ì…˜ì€ ìˆì§€ë§Œ ë²¡í„° ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ë¬¸ì„œ ì¶”ê°€
                    print("[INFO] ì»¬ë ‰ì…˜ì€ ì¡´ì¬í•˜ì§€ë§Œ ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    # ê¸°ì¡´ ë¬¸ì„œ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                    cur.execute(
                        f"""
                        SELECT DISTINCT document
                        FROM langchain_pg_embedding
                        WHERE collection_id = '{collection_uuid}'
                    """
                    )
                    existing_docs = {row[0] for row in cur.fetchall()}

                    initial_docs = [
                        Document(
                            page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
                            metadata={"source": "intro"},
                        ),
                        Document(
                            page_content="pgvectorëŠ” PostgreSQLì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í™•ì¥ì…ë‹ˆë‹¤.",
                            metadata={"source": "pgvector"},
                        ),
                        Document(
                            page_content="Hello WorldëŠ” í”„ë¡œê·¸ë˜ë°ì˜ ì²« ë²ˆì§¸ ì˜ˆì œì…ë‹ˆë‹¤.",
                            metadata={"source": "hello"},
                        ),
                    ]

                    # ì¤‘ë³µë˜ì§€ ì•Šì€ ë¬¸ì„œë§Œ ì¶”ê°€
                    docs_to_add = [
                        doc
                        for doc in initial_docs
                        if doc.page_content not in existing_docs
                    ]

                    if docs_to_add:
                        print(f"[INFO] ì´ˆê¸° ë¬¸ì„œ {len(docs_to_add)}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
                        vector_store.add_documents(docs_to_add)
                        print("[OK] ì´ˆê¸° ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
                    else:
                        print(
                            "[INFO] ì´ˆê¸° ë¬¸ì„œê°€ ì´ë¯¸ ëª¨ë‘ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                        )

                    # ì¶”ê°€ í›„ í™•ì¸
                    conn = psycopg2.connect(CONNECTION_STRING)
                    cur = conn.cursor()
                    cur.execute(
                        f"""
                        SELECT COUNT(*)
                        FROM langchain_pg_embedding
                        WHERE collection_id = '{collection_uuid}'
                    """
                    )
                    final_count = cur.fetchone()[0]

                    # ê³ ìœ  ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
                    cur.execute(
                        f"""
                        SELECT COUNT(DISTINCT document)
                        FROM langchain_pg_embedding
                        WHERE collection_id = '{collection_uuid}'
                    """
                    )
                    unique_doc_count = cur.fetchone()[0]
                    conn.close()
                    print(f"[INFO] í˜„ì¬ ë²¡í„° ë°ì´í„° ê°œìˆ˜: {final_count}ê°œ")
                    print(f"[INFO] ê³ ìœ  ë¬¸ì„œ ê°œìˆ˜: {unique_doc_count}ê°œ")
                    if final_count > unique_doc_count:
                        print(
                            f"[WARNING] ì¤‘ë³µëœ ë²¡í„°ê°€ {final_count - unique_doc_count}ê°œ ìˆìŠµë‹ˆë‹¤."
                        )
                    print("[OK] ===== PGVector ì—°ê²° í™•ì¸ ì™„ë£Œ =====")
            else:
                conn.close()
                print("[WARNING] ì»¬ë ‰ì…˜ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        except Exception as e:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ì´ˆê¸° ë¬¸ì„œë¡œ ìƒì„±
            error_msg = str(e)
            print("[INFO] ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
            print(f"[INFO] ì˜¤ë¥˜ ë‚´ìš©: {error_msg[:150]}")
            print("[INFO] ì´ˆê¸° ë¬¸ì„œë¡œ PGVector ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            vector_store = PGVector.from_documents(
                embedding=current_embeddings,
                documents=[
                    Document(
                        page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
                        metadata={"source": "intro"},
                    ),
                    Document(
                        page_content="pgvectorëŠ” PostgreSQLì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í™•ì¥ì…ë‹ˆë‹¤.",
                        metadata={"source": "pgvector"},
                    ),
                    Document(
                        page_content="Hello WorldëŠ” í”„ë¡œê·¸ë˜ë°ì˜ ì²« ë²ˆì§¸ ì˜ˆì œì…ë‹ˆë‹¤.",
                        metadata={"source": "hello"},
                    ),
                ],
                collection_name=COLLECTION_NAME,
                connection_string=CONNECTION_STRING,
            )
            print("[OK] PGVector ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")

            # ìƒì„± í›„ ë²¡í„° ê°œìˆ˜ í™•ì¸
            import psycopg2

            conn = psycopg2.connect(CONNECTION_STRING)
            cur = conn.cursor()
            cur.execute(
                f"""
                SELECT COUNT(*)
                FROM langchain_pg_embedding
                WHERE collection_id = (
                    SELECT uuid FROM langchain_pg_collection WHERE name = '{COLLECTION_NAME}'
                )
            """
            )
            vector_count = cur.fetchone()[0]
            conn.close()
            print(f"[INFO] ìƒì„±ëœ ë²¡í„° ë°ì´í„° ê°œìˆ˜: {vector_count}ê°œ")
            print("[OK] ===== PGVector ì—°ê²° í™•ì¸ ì™„ë£Œ =====")
    except Exception as e:
        error_msg = str(e)
        print(f"[ERROR] PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_msg[:200]}...")
        raise


def create_rag_chain(llm_model, embeddings_model):
    """RAG ì²´ì¸ ìƒì„± - LangChain ì²´ì¸ ê¸°ëŠ¥ í™œìš©."""
    try:
        # 1. Retriever ìƒì„± (í˜„ì¬ Embedding ëª¨ë¸ ì‚¬ìš©)
        current_vector_store = PGVector(
            embedding_function=embeddings_model,
            collection_name=COLLECTION_NAME,
            connection_string=CONNECTION_STRING,
        )
        retriever = current_vector_store.as_retriever(search_kwargs={"k": 3})

        # 2. ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
        contextualize_q_system_prompt = (
            "ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, "
            "ëŒ€í™” ê¸°ë¡ì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”. "
            "ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
        )
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        # 3. ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ Retriever ìƒì„±
        history_aware_retriever = create_history_aware_retriever(
            llm_model, retriever, contextualize_q_prompt
        )

        # 4. ì§ˆë¬¸ ë‹µë³€ í”„ë¡¬í”„íŠ¸
        qa_system_prompt = (
            "ë‹¹ì‹ ì€ LangChainê³¼ PGVectorë¥¼ ì‚¬ìš©í•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
            "ë‹¤ìŒ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. "
            "ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë©´, ì •ì¤‘í•˜ê²Œ ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”. "
            "ë‹µë³€ì€ ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            "ì»¨í…ìŠ¤íŠ¸:\n{context}"
        )
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )

        # 5. ë¬¸ì„œ ê²°í•© ì²´ì¸ ìƒì„±
        question_answer_chain = create_stuff_documents_chain(llm_model, qa_prompt)

        # 6. ìµœì¢… RAG ì²´ì¸ ìƒì„±
        rag_chain = create_retrieval_chain(
            history_aware_retriever, question_answer_chain
        )

        return rag_chain
    except Exception as e:
        error_msg = str(e)
        print(f"[ERROR] RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {error_msg[:200]}...")
        raise


def initialize_rag_chain():
    """RAG ì²´ì¸ ì´ˆê¸°í™” - OpenAIì™€ ë¡œì»¬ ëª¨ë¸ìš© ì²´ì¸ ìƒì„±."""
    global openai_rag_chain, local_rag_chain

    # OpenAIìš© RAG ì²´ì¸ ìƒì„±
    if openai_llm and openai_embeddings:
        try:
            openai_rag_chain = create_rag_chain(openai_llm, openai_embeddings)
            print("[OK] OpenAI RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"[WARNING] OpenAI RAG ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}...")
            openai_rag_chain = None

    # ë¡œì»¬ ëª¨ë¸ìš© RAG ì²´ì¸ ìƒì„±
    if local_llm and local_embeddings:
        try:
            local_rag_chain = create_rag_chain(local_llm, local_embeddings)
            print("[OK] ë¡œì»¬ RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"[WARNING] ë¡œì»¬ RAG ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}...")
            local_rag_chain = None

    if not openai_rag_chain and not local_rag_chain:
        error_msg = "OpenAIì™€ ë¡œì»¬ RAG ì²´ì¸ ëª¨ë‘ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
        if not openai_llm:
            error_msg += "- OpenAI LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        if not openai_embeddings:
            error_msg += "- OpenAI Embeddingsê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        if not local_llm:
            error_msg += "- ë¡œì»¬ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        if not local_embeddings:
            error_msg += "- ë¡œì»¬ Embeddingsê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        raise RuntimeError(error_msg)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”."""
    print("=" * 50)
    print("LangChain FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    print("=" * 50)

    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    llm_provider = os.getenv("LLM_PROVIDER", "openai").lower()
    local_model_dir = os.getenv("LOCAL_MODEL_DIR", "ê¸°ë³¸ê°’ ì‚¬ìš©")
    print(f"\n[INFO] LLM_PROVIDER: {llm_provider}")
    print(f"[INFO] LOCAL_MODEL_DIR: {local_model_dir}")

    # Neon PostgreSQL ì—°ê²° ëŒ€ê¸°
    print("\n1. Neon PostgreSQL ì—°ê²° í™•ì¸ ì¤‘...")
    wait_for_postgres()

    # Embedding ëª¨ë¸ ì´ˆê¸°í™”
    print("\n2. Embedding ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    initialize_embeddings()

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    print("\n3. LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    initialize_llm()

    # PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™”
    print("\n4. PGVector ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
    initialize_vector_store()

    # RAG ì²´ì¸ ì´ˆê¸°í™”
    print("\n5. RAG ì²´ì¸ ì´ˆê¸°í™” ì¤‘...")
    initialize_rag_chain()

    print("\n" + "=" * 50)
    print("[OK] ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
    print("=" * 50)


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    """ì±—ë´‡ ìš”ì²­ ëª¨ë¸."""

    message: str
    history: Optional[List[dict]] = []
    model_type: Optional[str] = "openai"  # "openai" ë˜ëŠ” "local"


class ChatResponse(BaseModel):
    """ì±—ë´‡ ì‘ë‹µ ëª¨ë¸."""

    response: str


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸."""
    return {
        "message": "LangChain Chatbot API",
        "status": "running",
        "docs": "/docs",
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸."""
    global openai_quota_exceeded
    return {
        "status": "healthy",
        "vector_store": "initialized" if vector_store else "not initialized",
        "openai_embeddings": "initialized" if openai_embeddings else "not initialized",
        "local_embeddings": "initialized" if local_embeddings else "not initialized",
        "openai_llm": "initialized" if openai_llm else "not initialized",
        "local_llm": "initialized" if local_llm else "not initialized",
        "openai_rag_chain": "initialized" if openai_rag_chain else "not initialized",
        "local_rag_chain": "initialized" if local_rag_chain else "not initialized",
        "openai_quota_exceeded": openai_quota_exceeded,
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ì±—ë´‡ API ì—”ë“œí¬ì¸íŠ¸ - LangChain RAG ì²´ì¸ ì‚¬ìš©."""
    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ RAG ì²´ì¸ ì„ íƒ
    # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ëœ model_typeì´ ì—†ìœ¼ë©´ .envì˜ LLM_PROVIDER ì‚¬ìš©
    model_type = request.model_type or os.getenv("LLM_PROVIDER", "openai")
    if model_type:
        model_type = model_type.lower()

    # ë””ë²„ê¹…: ë°›ì€ model_type ë¡œê·¸ ì¶œë ¥
    print(
        f"[DEBUG] ë°›ì€ model_type: {request.model_type}, ì²˜ë¦¬ëœ model_type: {model_type}"
    )

    # "midm"ë„ "local"ë¡œ ì²˜ë¦¬
    if model_type == "midm":
        model_type = "local"

    if model_type == "openai":
        if not openai_rag_chain:
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸
            global openai_quota_exceeded

            if openai_quota_exceeded:
                # í• ë‹¹ëŸ‰ ì´ˆê³¼ì¸ ê²½ìš° ëª…í™•í•œ ë©”ì‹œì§€
                error_msg = (
                    "âš ï¸ OpenAI API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                    "ì„œë²„ ì‹œì‘ ì‹œ '[WARNING] OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼' ë©”ì‹œì§€ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                    "í•´ê²° ë°©ë²•:\n"
                    "1. OpenAI ê³„ì •ì˜ ì‚¬ìš©ëŸ‰ ë° í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ì„¸ìš”\n"
                    "2. OpenAI ê³„ì •ì— ê²°ì œ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ í• ë‹¹ëŸ‰ì„ ëŠ˜ë¦¬ì„¸ìš”\n"
                    "3. ë˜ëŠ” 'ğŸ–¥ï¸ ë¡œì»¬ ëª¨ë¸' ë²„íŠ¼ì„ ì„ íƒí•˜ì—¬ ë¡œì»¬ Midm ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”"
                )
            elif not openai_llm and not openai_embeddings:
                # ë‘˜ ë‹¤ ì´ˆê¸°í™” ì‹¤íŒ¨ (í• ë‹¹ëŸ‰ ì´ˆê³¼ê°€ ì•„ë‹Œ ê²½ìš°)
                error_msg = (
                    "OpenAI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
                    "ê°€ëŠ¥í•œ ì›ì¸:\n"
                    "1. OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤\n"
                    "2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ\n\n"
                    "í•´ê²° ë°©ë²•:\n"
                    "- .env íŒŒì¼ì— ì˜¬ë°”ë¥¸ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”\n"
                    "- ë˜ëŠ” 'ë¡œì»¬ ëª¨ë¸' ë²„íŠ¼ì„ ì„ íƒí•˜ì—¬ ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”"
                )
            else:
                # ì¼ë¶€ë§Œ ì‹¤íŒ¨
                error_details = []
                if not openai_llm:
                    error_details.append("OpenAI LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                if not openai_embeddings:
                    error_details.append("OpenAI Embeddingsê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                error_msg = f"OpenAI RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {', '.join(error_details)}"

            print(f"[ERROR] OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œë„ ì‹¤íŒ¨: {error_msg}")
            raise HTTPException(
                status_code=503,
                detail=error_msg,
            )
        current_rag_chain = openai_rag_chain
    elif model_type == "local" or model_type == "midm":
        if not local_rag_chain:
            raise HTTPException(
                status_code=503,
                detail="ë¡œì»¬ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Midm ëª¨ë¸ê³¼ sentence-transformersë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            )
        print(f"[DEBUG] ë¡œì»¬ RAG ì²´ì¸ ì‚¬ìš© (model_type: {model_type})")
        current_rag_chain = local_rag_chain
    else:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: {model_type}. 'openai' ë˜ëŠ” 'local'ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
        )

    try:
        # ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chat_history = []
        if request.history:
            for msg in request.history:
                if msg.get("role") == "user":
                    chat_history.append(HumanMessage(content=msg.get("content", "")))
                elif msg.get("role") == "assistant":
                    chat_history.append(AIMessage(content=msg.get("content", "")))

        # RAG ì²´ì¸ ì‹¤í–‰
        result = current_rag_chain.invoke(
            {
                "input": request.message,
                "chat_history": chat_history,
            }
        )

        # ì²´ì¸ ê²°ê³¼ì—ì„œ ë‹µë³€ ì¶”ì¶œ
        response_text = result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # response_textê°€ Noneì´ê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
        if response_text is None:
            response_text = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            response_text = str(response_text)

        # ì‘ë‹µì—ì„œ ì´ì „ ëŒ€í™” ë‚´ìš© ì œê±° (ì¤‘ë³µ ë°©ì§€)
        # Midm ëª¨ë¸ì—ì„œ ì´ë¯¸ ì •ë¦¬í–ˆìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ì²´í¬ë§Œ ìˆ˜í–‰
        if response_text and (
            "Human:" in response_text or "Assistant:" in response_text
        ):
            import re

            # ë¹ ë¥¸ ì •ê·œì‹ìœ¼ë¡œ ë§ˆì§€ë§‰ Assistant: ì´í›„ë§Œ ì¶”ì¶œ
            assistant_match = re.search(
                r"Assistant:\s*(.+?)(?:\nHuman:|$)", response_text, re.DOTALL
            )
            if assistant_match:
                response_text = assistant_match.group(1).strip()

        # ë¹ˆ ì‘ë‹µ ë°©ì§€
        if not response_text or not response_text.strip():
            response_text = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return ChatResponse(response=response_text)

    except Exception as e:
        error_msg = str(e)
        print(f"[ERROR] ì±—ë´‡ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {error_msg}")

        # OpenAI API í˜¸ì¶œëŸ‰ ì´ˆê³¼ ì—ëŸ¬ í™•ì¸
        if (
            "quota" in error_msg.lower()
            or "429" in error_msg
            or "insufficient_quota" in error_msg
            or "exceeded" in error_msg.lower()
        ):
            error_detail = "OpenAI API í˜¸ì¶œëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. í• ë‹¹ëŸ‰ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            raise HTTPException(
                status_code=429,
                detail=error_detail,
            )
        else:
            raise HTTPException(
                status_code=500,
                detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg[:200]}",
            )


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
